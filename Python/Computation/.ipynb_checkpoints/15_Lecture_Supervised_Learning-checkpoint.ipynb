{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "764b65ca-08e5-41b8-95bd-4fc93064b8b5",
   "metadata": {},
   "source": [
    "# Supervised Learning with Feedforward Neural Networks\r\n",
    "\r\n",
    "Supervised learning is one of the foundational ideas in machine learning. If you've heard of machine learning before, chances are you're thinking of this type. In supervised learning, the key idea is that we teach a model by example.\r\n",
    "\r\n",
    "We start with a dataset that includes both inputs (also called *features*) and the corresponding outputs (also called *labels* or *targets*). The goal is to learn the relationship between the inputs and outputs so that, when we're later given a new input, the model can correctly predict the output.\r\n",
    "\r\n",
    "Imagine a few concrete examples:\r\n",
    "- You have thousands of images of handwritten digits, and each image is labeled with the correct digit (0 through 9). You want your model to learn how to read digits, so it can correctly classify new, unlabeled images.\r\n",
    "- You have a dataset of house features (square footage, number of bedrooms, location, etc.) and the prices those houses sold for. You want to predict the sale price of a new house.\r\n",
    "- You have audio clips and the corresponding transcripts. You want to build a system that listens to an audio clip and transcribes the words.\r\n",
    "\r\n",
    "In all these cases, we're learning from examples with known answers—that's why it's called *supervised* learning. We’re supervising the model by telling it the correct answer during training.\r\n",
    "\r\n",
    "## What Does the Model Learn?\r\n",
    "\r\n",
    "The job of a supervised learning model is to discover patterns in the training data—patterns that connect inputs to outputs. The more representative and diverse the training data is, the better the model will be at generalizing to new, unseen examples.\r\n",
    "\r\n",
    "One common type of supervised learning model is the **feedforward neural network**. This kind of model mimics, in a very simplified way, the way biological neurons work. Information flows through the network in one direction—from input to output—passing through layers of artificial “neurons” that apply weighted transformations and nonlinear functions.\r\n",
    "\r\n",
    "During training, the network adjusts its internal weights to minimize the difference between its predictions and the true outputs. This process is called *learning*, and it's usually done using a method called *gradient descent*, combined with an algorithm called *backpropagation* that efficiently computes how the weights should be updated.\r\n",
    "\r\n",
    "## Two Major Subtypes: Classification and Regression\r\n",
    "\r\n",
    "There are two broad categories of problems in supervised learning, and knowing the difference is crucial:\r\n",
    "\r\n",
    "- **Classification:** Here, the outputs are *discrete categories*. The goal is to assign an input to one of a fixed set of categories. For example:\r\n",
    "  - Is this tumor benign or malignant?\r\n",
    "  - What digit is in this image?\r\n",
    "  - Which particle is observed in this experiment?\r\n",
    "\r\n",
    "  In these problems, the output is often represented as a one-hot vector—an array where only one element is “on” (e.g., `[0, 0, 1, 0]` for class 2).\r\n",
    "\r\n",
    "- **Regression:** In these problems, the output is a *continuous value*. You're predicting something that varies smoothly. For example:\r\n",
    "  - What is the temperature in Boston tomorrow?\r\n",
    "  - What is the energy of a particle given its momentum?\r\n",
    "  - How much will this apartment rent for?\r\n",
    "\r\n",
    "  In regression, the model’s outputs are real numbers, and the performance is often measured by metrics like mean squared error.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca06779-87a7-4749-8217-d564f03043c4",
   "metadata": {},
   "source": [
    "# Supervised Learning Demo with scikit-learn\r\n",
    "\r\n",
    "In this notebook, we'll use Python's `scikit-learn` library to perform supervised learning on a classic dataset: the handwritten digits dataset.\r\n",
    "\r\n",
    "We’ll:\r\n",
    "- Load a dataset of labeled examples (images of digits)\r\n",
    "- Train a model to recognize the digits\r\n",
    "- Evaluate its accuracy\r\n",
    "- Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f8b54-c3e8-41bc-be97-88d752cbc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Show some example images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {digits.target[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307bfe0-dee2-4547-956b-c0116b21c5d6",
   "metadata": {},
   "source": [
    "## Step 1: Split the Data\r\n",
    "\r\n",
    "We'll divide the data into a **training set** and a **test set**. The model will learn from the training set and be evaluated on the test set.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32652af-ea3d-44e1-8b10-82aba6722ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 80% train and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186e225-b16f-4c81-bb0d-155a28db2a96",
   "metadata": {},
   "source": [
    "## Step 2: Train a Model\r\n",
    "\r\n",
    "We'll use logistic regression, a simple but effective classifier, to train the model on the training dat\n",
    "\n",
    "Some other classifiers include Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Random Forest.a.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25a4d1-a0bf-4f18-965f-cd45c2b777b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=5000)  # max_iter increased to ensure convergence\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441de6f-74e8-4e35-964f-0ea775f41905",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions and Evaluate\n",
    "\n",
    "Let’s see how well our model does on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5a8b5-dfe0-43a9-b3d6-b2ee95f254a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Check accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Show a detailed classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338738d7-ca6c-4d93-9279-8b7a22729893",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Predictions\n",
    "\n",
    "Let's look at some test images alongside the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0270b-f510-47bf-af9e-356b5b47c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 10 test images with their predicted labels\n",
    "offset = 101\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X_test[i+offset].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {y_pred[i+offset]}, Truth: {y_test[i+offset]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723c4a2-4f23-4479-bc88-aea769dacf8b",
   "metadata": {},
   "source": [
    "# From scikit-learn to PyTorch\r\n",
    "\r\n",
    "So far, we've used `scikit-learn`, a powerful and beginner-friendly library for supervised learning. It abstracts away many of the technical details, allowing us to train and evaluate models with just a few lines of code. This makes it ideal for quickly applying standard algorithms like logistic regression, decision trees, or support vector machines.\r\n",
    "\r\n",
    "However, if we want to dive deeper—especially into neural networks and deep learning—we need more flexibility and control. That's where `PyTorch` comes in. PyTorch allows us to build models from scratch, define custom architectures, and see exactly how data flows through the network. It requires a bit more setup, but it gives us full access to the inner workings of training algorithms.\r\n",
    "\r\n",
    "Let’s now shift to using PyTorch to build and train a simple feedforward neural network on the same digits dataset.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723a0ee-2ce8-4b85-9bc3-748aa1bf6f55",
   "metadata": {},
   "source": [
    "## Classifying Digits with PyTorch: Step-by-Step\r\n",
    "\r\n",
    "Now that we've used `scikit-learn` for supervised learning, let’s walk through how to build and train a neural network from scratch using PyTorch. We’ll use the same digits dataset and mirror the steps we followed earlier.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Step 1: Load and Preprocess the Data\r\n",
    "\r\n",
    "We’ll load the digits dataset, normalize the features, and convert everything into tensors. Pytorch Tensors are the base datatype used to analyze and pass data in tensors.ensors.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792041f5-42d6-4e1e-9ab1-536ed77ed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c446bc-75c8-4f2e-9177-4a410b17c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d7ab32-a0ad-4b17-80e7-d3aacaf85827",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### Step 2: Define the Neural Network\r\n",
    "\r\n",
    "We’ll use a simple feedforward architecture with one hidden layer and ReLU activation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c81a-a939-43d2-9653-99b63c1e75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(64, 100),  # 64 input features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10)   # 10 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = DigitClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24290596-7cb3-4091-a897-8b2e93ac84e6",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### Step 3: Set Up the Loss Function and Optimizer\r\n",
    "\r\n",
    "We'll use cross-entropy loss for classification, and the Adam optimizer for train The cross-entropy loss is used in classification schemes, especially when the model outputs probabilities. It measures the difference between two probability distributions: the true labels and the predicted probabilities of the model. If the model assigns high probability to the correct answer, the loss is small. If the model assigns low probability to the correct class, the loss is large, punishing confident wrong answer more harshly.ing.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a978e97-18e4-40c5-8d5b-3fa5a0409001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee7d17-e6e7-4144-8196-ec7fb06cd890",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### Step 4: Train the Model\r\n",
    "\r\n",
    "We’ll loop over the training data for several epochs, computing the loss and updating the weights using backpropagation.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e06798-3b2d-4560-aa7f-4e49599fa5f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "epochs = 20\n",
    "lossacct = []\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "    lossacct.append(total_loss)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lossacct)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# Force integer ticks on the x-axis\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45077eee-e81f-4871-ad6d-d6983d1ffd7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 5: Evaluate the Model\n",
    "\n",
    "Let’s measure accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdde504-30b9-4c66-ab2a-e9e644be5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131cdc4-223f-48be-ad2a-5bbfc815c957",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "### Step 6: Visualize Predictions\r\n",
    "\r\n",
    "We'll look at a few test samples and their predicted labels.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf617d9-1517-4f10-8bcd-0980fb4bcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X_test[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {all_preds[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5fe87-6c90-4a0d-831a-88679ca371bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f69cf-7eca-4433-9eda-8900279d0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d27eb-5000-4c2f-9f88-9a74db9f6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the weights of the first layer\n",
    "input_weights = model.net[0].weight.data\n",
    "print(input_weights)\n",
    "\n",
    "# Biases of the output layer\n",
    "output_bias = model.net[2].bias.data\n",
    "print(output_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7a972-0a73-4965-81be-750ecfdefab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542e1f6-227a-46c2-be67-1b07c5d16804",
   "metadata": {},
   "source": [
    "# Regression: $Z \\to e^+ e^-$ decays\n",
    "\n",
    "In this problem we'll use CERN OpenData taken from the website [opendata.cern.ch](http://opendata.cern.ch/record/545) to train a classifier to distinguish between $Z \\to e^+ e^-$ decays and background events. The data is stored in the file `Zee.csv` and contains the following variables:\n",
    "- `E1`, `E2`: The energy of the two electrons in GeV\n",
    "- `pt1`, `pt2`: The transverse momentum of the two electrons in GeV\n",
    "- `eta1`, `eta2`: The pseudorapidity of the two electrons\n",
    "- `phi1`, `phi2`: The azimuthal angle of the two electrons\n",
    "and some other data that won't be as relevant for us.\n",
    "\n",
    "We will train a neural network to predict the invariant mass of the Z-boson from the transverse momenta `pt`, the pseudo-rapidities `eta` and the azimuthal angles `phi` of the electron and positron. The pseudo-rapidity is given by\n",
    "$$\\eta = -\\ln(\\tan(\\theta/2))$$\n",
    "where $\\theta$ is the angle between the particle's momentum and the beam axis.\n",
    "Of course, the exact formula for the invariant mass is known, but this is useful pedagogically because it will give us some confidence in the neural network.\n",
    "\n",
    "In terms of these variables, the formula for the reconstructed invariant mass of the $Z$-boson is \n",
    "$M_Z^2 = 2 p_{T,1} p_{T,2} ( \\cosh(\\eta_1 - \\eta_2) - \\cos(\\phi_1 - \\phi_2))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d285408-2167-439a-873b-7762097d92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Zee.csv')\n",
    "\n",
    "# Combine electron and positron kinematics into one DataFrame\n",
    "particles = pd.DataFrame({\n",
    "    'pt': pd.concat([data['pt1'], data['pt2']]),\n",
    "    'eta': pd.concat([data['eta1'], data['eta2']]),\n",
    "    'charge': pd.concat([data['Q1'], data['Q2']])\n",
    "})\n",
    "\n",
    "# Randomly select 1000 particles\n",
    "random_indices = random.sample(range(len(particles)), 3000)\n",
    "sample_data = particles.iloc[random_indices]\n",
    "\n",
    "# Create a 2D scatter plot of pt vs eta\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    sample_data['eta'],\n",
    "    sample_data['pt'],\n",
    "    c=sample_data['charge'],\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.7,\n",
    "    s=40\n",
    ")\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Eta (pseudorapidity)')\n",
    "plt.ylabel('PT (transverse momentum) [GeV/c]')\n",
    "plt.title('Transverse Momentum vs Pseudorapidity in $Z \\\\rightarrow e^+e^-$ Events')\n",
    "\n",
    "# Colorbar to show charge\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Charge')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print simple statistics\n",
    "print(f\"PT range: {sample_data['pt'].min():.2f} to {sample_data['pt'].max():.2f} GeV/c\")\n",
    "print(f\"Eta range: {sample_data['eta'].min():.2f} to {sample_data['eta'].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ba279-a0d3-4c14-8788-6b76724aadd9",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 1: Load and Prepare the data \n",
    "We'll load teh data from `Zee.csv`, extract the features, and compute the invariant mass from the exact formula to use as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b0be3-db68-4cd2-a045-1e73bd30920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Zee.csv')\n",
    "\n",
    "# Select kinematic variables\n",
    "features = ['pt1', 'pt2', 'eta1', 'eta2', 'phi1', 'phi2']\n",
    "X = df[features].values\n",
    "\n",
    "# Compute the invariant mass using the given formula\n",
    "pt1, pt2 = df['pt1'], df['pt2']\n",
    "eta1, eta2 = df['eta1'], df['eta2']\n",
    "phi1, phi2 = df['phi1'], df['phi2']\n",
    "\n",
    "mass_squared = 2 * pt1 * pt2 * (np.cosh(eta1 - eta2) - np.cos(phi1 - phi2))\n",
    "mass = np.sqrt(np.abs(mass_squared))  # abs to prevent tiny negatives from sqrt\n",
    "\n",
    "y = mass.values.reshape(-1, 1)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861dcce-722a-427c-a2bf-1ed63e6e2ee4",
   "metadata": {},
   "source": [
    "------------\n",
    "## Step 2: Normalize the inputs\n",
    "We normalize the input features using `StandardScaler` to have zero mean and unit variance. This helps the neural network train faster and more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac636ad8-3e80-4cb1-871d-c9daa1143ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5217a682-8959-458b-8996-fb89616b4b96",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Step 3: Define the Neural Network\n",
    "We'll use a simple feedforward netowrk with two hidden layers. Since this is a regression problem, the output layer has a single neuron with no activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966450b-65fe-44d9-8947-eead0faedae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ZMassRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(6, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Output: predicted mass\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ZMassRegressor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4748499-09e2-4299-a54a-24cb3399d11a",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 4: Set Up the Loss Function and Optimizer\n",
    "We'll use Mean Square Error (MSE) as the loss, which is standard for regression problems. The Adam opitmizer will update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd17ac2-a386-4ab6-a4b3-148f284cf946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd23280-beef-4829-9b7b-cbb25a20b0c1",
   "metadata": {},
   "source": [
    "-----\n",
    "## Step 5: Train the Model\n",
    "Fro each epoc, we loop over training batches, perform forward and backward passes, and update weights to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb43de1-7d43-4c3c-b590-dc5436ceb238",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0642d-f284-4c44-959f-0883fecad18b",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the Model\n",
    "Now we compare the predicted masses to the true masses on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5571e-4e38-4fc4-9ba0-e94d565bd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy()\n",
    "\n",
    "# Flatten for plotting\n",
    "y_true = y_test.flatten()\n",
    "y_pred_flat = y_pred.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27426142-5b8d-48aa-9a2b-f916d02a4ff6",
   "metadata": {},
   "source": [
    "-------\n",
    "## Step 7: Visualize Predictions\n",
    "Let's compare true and predicted invariant masses with a scatter plot. A perfect prediction would fall on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cc26e-2f02-4cd8-8c47-76bd9a60dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true, y_pred_flat, alpha=0.5)\n",
    "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "plt.xlabel(\"True $M_Z$ [GeV]\")\n",
    "plt.ylabel(\"Predicted $M_Z$ [GeV]\")\n",
    "plt.title(\"Z Boson Mass Prediction\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc76195-94eb-4717-a0cc-1a619d81d12b",
   "metadata": {},
   "source": [
    "## Step 8: Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfadbb2-7740-4233-bc85-7dc604ddd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (MSE)\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e5bb9-40c3-4076-b139-fb77925e22df",
   "metadata": {},
   "source": [
    "## Learning about the Model\n",
    "Since our model takes 6 input variables, we cannot plot the learned function in 2D. But we can fix all but one or two of the inputs and see how the predicted mass depends on those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15e68d-e0a1-4ad3-abdc-23cb2e35366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fix all inputs except pt1\n",
    "baseline = X_test_tensor[0].clone().numpy()\n",
    "pt1_range = np.linspace(-2, 3, 100)  # Still in standardized units\n",
    "\n",
    "# Vectorized creation of inputs\n",
    "inputs_array = np.tile(baseline, (len(pt1_range), 1))\n",
    "inputs_array[:, 0] = pt1_range  # Replace pt1 with values from range\n",
    "\n",
    "# Now convert efficiently to torch tensor\n",
    "inputs_tensor = torch.tensor(inputs_array, dtype=torch.float32)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs_tensor).numpy().flatten()\n",
    "\n",
    "# Plot\n",
    "plt.plot(pt1_range, predictions)\n",
    "plt.xlabel(\"Standardized $p_{T1}$\")\n",
    "plt.ylabel(\"Predicted $M_Z$ [GeV]\")\n",
    "plt.title(\"Effect of $p_{T1}$ on Predicted Z Mass\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164ee3a-083f-4401-8da9-afaeedbe8cd3",
   "metadata": {},
   "source": [
    "## Inspecting Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be2242-a8fc-4fb6-a49f-67a5c132e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all parameter names and shapes\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ccb57-77ff-4f81-9aad-c9d14c42afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from input layer\n",
    "input_weights = model.net[0].weight.data.numpy()\n",
    "print(\"Input layer weights shape:\", input_weights.shape)\n",
    "\n",
    "# Optionally, visualize weights as a heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(input_weights, cmap=\"coolwarm\", xticklabels=features, yticklabels=False)\n",
    "plt.title(\"Weights from Input Features to First Hidden Layer\")\n",
    "plt.xlabel(\"Input Feature\")\n",
    "plt.ylabel(\"Hidden Neuron\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c329e-f1c2-4d6d-af3a-64f355908dea",
   "metadata": {},
   "source": [
    "## SHAP Values\n",
    "Hey, remember learning about SHAP values. Let's see them in action here when trying to see what inputs were most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca2284-8cde-4c52-aacd-00abf864cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define a wrapper for the PyTorch model to work with SHAP\n",
    "def model_fn(x_numpy):\n",
    "    x_tensor = torch.tensor(x_numpy, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        return model(x_tensor).numpy()\n",
    "\n",
    "# Use a subset of the training data as the SHAP background\n",
    "explainer = shap.Explainer(model_fn, X_train, feature_names=features)\n",
    "\n",
    "# Compute SHAP values on test data\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot (beeswarm)\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfeb88-3314-4203-b082-027abf886476",
   "metadata": {},
   "source": [
    "## Learning Rate Determination?\n",
    "There is a method to help determine the optimal learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08531c7b-ea62-4a80-9e26-041973e46eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the model class\n",
    "class ZMassRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(6, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Prepare data (reuse your X_train, y_train from earlier)\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Try a range of learning rates\n",
    "lrs = np.logspace(-5, -1, 10)\n",
    "losses = []\n",
    "\n",
    "for lr in lrs:\n",
    "    model = ZMassRegressor()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(lrs, losses, marker='o')\n",
    "plt.xlabel(\"Learning Rate (log scale)\")\n",
    "plt.ylabel(\"Training Loss (1 mini-epoch)\")\n",
    "plt.title(\"Learning Rate Sweep\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68b11a-0a32-4edb-9186-26769904f58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
