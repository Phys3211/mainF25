{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be70c57-9cbd-42e2-a67f-8836d9eb9e30",
   "metadata": {},
   "source": [
    "# Introduction to Monte Carlo Methods in Computational Science\r\n",
    "\r\n",
    "## Overview\r\n",
    "\r\n",
    "Monte Carlo methods are a powerful class of numerical techniques that leverage randomness to solve deterministic problems. These methods are widely used in physics, statistics, and computational science for tasks such as numerical integration, optimization, and probabilistic modeling.\r\n",
    "\r\n",
    "In this notebook, we will explore the foundational concepts of Monte Carlo methods, including:\r\n",
    "\r\n",
    "- Random number generation and how probability distributions play a role in sampling.\r\n",
    "- Von Neumann rejection sampling, a technique for drawing random numbers from a complex probability distribution.\r\n",
    "- The Metropolis algorithm, a Markov Chain Monte Carlo (MCMC) method for generating samples from an arbitrary probability distribution.\r\n",
    "- Monte Carlo integration, a method for approximating integrals using random sampling.\r\n",
    "- Importance sampling, an optimization technique that improves the efficiency of Monte Carlo integration by reducing variance.\r\n",
    "\r\n",
    "## Why Monte Carlo Methods?\r\n",
    "\r\n",
    "Many integrals and summations in physics and computational science are either high-dimensional or analytically intractable. Monte Carlo methods provide a way to approximate these computations using random sampling. The advantage of Monte Carlo approaches is their ability to handle high-dimensional spaces more effectively than traditional numerical quadrature methods.\r\n",
    "\r\n",
    "## Roadmap\r\n",
    "\r\n",
    "We will first examine how to generate random numbers from different distributions, then introduce the concept of rejection sampling as a method for drawing samples from non-trivial distributions. Building on this, we will explore the Metropolis algorithm, which extends these ideas into an iterative sampling technique. Finally, we will apply these methods to Monte Carlo integration, both with and without importance sampling, to illustrate how we can efficiently estimate integrals that are difficult to evaluate using traditional methods.\r\n",
    "\r\n",
    "# Random Number Generation and Pseudorandom Number Generators\r\n",
    "\r\n",
    "## Random Sequences\r\n",
    "Computers are deterministic machines, making it impossible to generate truly random numbers. Instead, they use **pseudorandom number generators (PRNGs)**, which produce sequences of numbers that appear random but are generated by deterministic algorithms.\r\n",
    "\r\n",
    "A sequence of numbers `{r_1, r_2, ... r_n}` is considered **random** if:\r\n",
    "- There are no correlations between the numbers.\r\n",
    "- The sequence follows a probability distribution $ P(r) $.\r\n",
    "- It satisfies properties such as **unpredictability, independence, and absence of patterns**.\r\n",
    "\r\n",
    "For example, a **uniformly distributed** sequence means that all numbers have an equal probability of appearing, represented as $ P(r) = 1 $ for a standard uniform distribution in $ [0,1] $.\r\n",
    "\r\n",
    "## Pseudorandom Number Generators (PRNGs)\r\n",
    "Since true randomness is not achievable in computers, PRNGs generate sequences using deterministic mathematical formulas. One common approach is the **Linear Congruential Generator (LCG)**, which follows the recurrence relation:\r\n",
    "\r\n",
    "$$\r\n",
    "r_{i+1} = (a r_i + c) \\mod M\r\n",
    "$$\r\n",
    "\r\n",
    "where:\r\n",
    "- $ r_1 $ is the **seed**, which initializes the sequence,\r\n",
    "- $ a $, $ c $, and $ M $ are carefully chosen constants,\r\n",
    "- The sequence has a periodicity of at most $ M $.\r\n",
    "\r\n",
    "### Example of an LCG\r\n",
    "Choosing $ c = 1 $, $ a = 4 $, $ M = 9 $, and $ r_1 = 3 $, the generated sequence cycles every 9 steps.\r\n",
    "\r\n",
    "$$\r\n",
    "\\begin{aligned}\r\n",
    "r_1 &= 3, \\\\\r\n",
    "r_2 &= (4 \\times 3 + 1) \\mod 9 = 4, \\\\\r\n",
    "r_3 &= (4 \\times 4 + 1) \\mod 9 = 8, \\\\\r\n",
    "r_4 &= (4 \\times 8 + 1) \\mod 9 = 6, \\\\\r\n",
    "&\\quad 7, 2, 0, 1, 5, 3, \\dots\r\n",
    "\\end{aligned}\r\n",
    "$$\r\n",
    "\r\n",
    "The period of an LCG depends on the choice of constants and follows conditions from the **Hull-Dobell Theorem**. The theorem states if and only if 3 conditions are met, will the period of the LCG be equal to $M$:\r\n",
    "1. $m$ and $c$ are coprime\r\n",
    "2. $a-1$ is divisible by all the prime factors of $M$\r\n",
    "3. $a-1$ is divisible by 4 if m is divisible by 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647c95e-bae0-4671-b7e3-e94fb06ac5b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N = 100000\n",
    "r = np.zeros(N+1)\n",
    "M = 6537169170218971\n",
    "a = 52361276312121\n",
    "\n",
    "seed = 12391.\n",
    "r[0] = seed\n",
    "for i in range(1, N+1):\n",
    "    r[i] = (a*r[i-1]) % M\n",
    "\n",
    "r1 = np.zeros(N//2)\n",
    "r2 = np.zeros(N//2)\n",
    "for i in range(0,N,2):\n",
    "    r1[i//2] = float(r[i])/float(M)\n",
    "    r2[i//2] = float(r[i+1])/float(M)\n",
    "\n",
    "plt.plot(r1, r2, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e8ee8-aea0-4a03-b014-2846c946676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_r = r/float(M)\n",
    "# histogram\n",
    "plt.figure()\n",
    "plt.hist(normalized_r, bins=100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c34b2-0279-4793-a6a6-aa897ab2e6d2",
   "metadata": {},
   "source": [
    "## Limitations of Simple PRNGs\r\n",
    "- **Short cycles:** Poor choices of parameters can result in sequences repeating too quickly.\r\n",
    "- **Correlations:** Some PRNGs, like the infamous **RANDU**, were flawed and led to incorrect scientific results.\r\n",
    "- **Determinism:** Knowing one number can reveal the entire sequence.\r\n",
    "\r\n",
    "To improve randomness, modern PRNGs use more sophisticated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61554948-ca29-42e6-8929-f9f0814b33b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N = 100000\n",
    "r = np.zeros(N+1)\n",
    "# This the infamous RANDU\n",
    "M = \n",
    "a = \n",
    "\n",
    "seed = 12391.\n",
    "r[0] = seed\n",
    "for i in range(1, N+1):\n",
    "    r[i] = (a*r[i-1]) % M\n",
    "\n",
    "r1 = np.zeros(N//2)\n",
    "r2 = np.zeros(N//2)\n",
    "for i in range(0,N,2):\n",
    "    r1[i//2] = float(r[i])/float(M)\n",
    "    r2[i//2] = float(r[i+1])/float(M)\n",
    "\n",
    "plt.plot(r1, r2, 'o')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953acfee-54fc-4177-82fe-daed2d94ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_r = r/float(M)\n",
    "# histogram\n",
    "plt.figure()\n",
    "plt.hist(normalized_r, bins=100, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a9f67-ec9b-4be2-9e1f-d4915b93f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "r1 = np.zeros(int(N/3))\n",
    "r2 = np.zeros(int(N/3))\n",
    "r3 = np.zeros(int(N/3))\n",
    "\n",
    "for i in range(0,N-1,3):\n",
    "    r1[int(i/3)] = r[i]/float(M)\n",
    "    r2[int(i/3)] = r[i+1]/float(M)\n",
    "    r3[int(i/3)] = r[i+2]/float(M)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.view_init(elev=20., azim=0)\n",
    "\n",
    "ax.scatter(r1,r2,r3,marker=\"o\"); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ef647-e333-4b2c-82e0-3019fa1803b3",
   "metadata": {},
   "source": [
    "\n",
    "\r",
    "## The Mersenne Twister\r\n",
    "A widely used PRNG in scientific computing is the **Mersenne Twister (MT19937)**. It has several advantages:\r\n",
    "- **Extremely long period** ($ 2^{19937}-1 $), preventing short cycles.\r\n",
    "- **Good statistical properties**, passing most randomness tests.\r\n",
    "- **Efficient generation**, making it ideal for Monte Carlo simulations.\r\n",
    "\r\n",
    "Python’s `random` module and NumPy’s `numpy.random` module use variations of the Mersenne Twister, making it the standard for many Monte Carlo applications.\r\n",
    "\r\n",
    "\r\n",
    "## Key Desirable Properties for PRNGs\r\n",
    "For Monte Carlo simulations, an ideal PRNG should be:\r\n",
    "- **Efficient**: Able to generate many numbers quickly.\r\n",
    "- **Long-period**: Avoiding early repetition.\r\n",
    "- **Repeatable**: For debugging purposes.\r\n",
    "- **Resumable**: Allowing simulations to pick up where they left off.\r\n",
    "- **Splittable**: Allowing independent parallel random streams.\r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e37a25-d250-4fb3-a4c7-d4179f4fa77e",
   "metadata": {},
   "source": [
    "# Non-uniform random distributions\n",
    "\n",
    "In the previous section we learned how to generate random deviates with a uniform probability distribution in an interval $[a,b]$. This distributioon is normalized, so that $$\\int _a^b {P(x)dx}=1.$$ Hence, $P(x)=1/(b-a)$.\n",
    "\n",
    "Now, suppose that we generate a sequence $\\{x_i\\}$ and we take some function of it to generate $\\{y(x_i)\\}=\\{y_i\\}$. This new sequence is going to be distributed according to some probability density $P(y)$, such that $$P(y)dy=P(x)dx$$ or $$P(y)=P(x)\\frac{dx}{dy}.$$\n",
    "\n",
    "If we want to generate a desired normalized distribution $P(y)$, we need to solve the differential equation: \n",
    "$$\\frac{dx}{dy}=P(y).$$ \n",
    "But the solution of this is $$x=\\int _0^y {P(y')dy'}=F(y),$$ where $F(y)$ is known as the cumulative distribution function (CDF).\n",
    "\n",
    "Therefore, \n",
    "$$y(x)=F^{-1}(x),$$ \n",
    "where $F^{-1}$ is the inverse of $F$.\n",
    "\n",
    "### Exponential distribution\n",
    "\n",
    "As an example, let us take $P(x)$ representing a uniform distribution in the interval $[0,1]$. We want to sample from\n",
    "$$P(y)=\\frac{dx}{dy}=e^{-y},$$  \n",
    "\n",
    "To figure out the relevant transformation, we compute the CDF \n",
    "$$x=F(y)=\\int _0^y {e^{-y'}dy'}=1-e^{-y}.$$\n",
    "Solving for $y$ in terms of $x$, we have \n",
    "$$y=F^{-1}(x)=-\\ln{(1-x)}.$$\n",
    "This distribution occurs frequently in real problems such as the radioactive decay of nuclei. You can also see that the quantity $y/\\lambda$ has the\n",
    "distribution $\\lambda e^{-\\lambda y}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89503ee-7fe8-4b58-bc13-de6735276726",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N = 1000\n",
    "x = np.random.random(N)\n",
    "\n",
    "lam = 0.1\n",
    "#Transformation equation\n",
    "y = -np.log(1-x)/lam\n",
    "\n",
    "binwidth=lam*5\n",
    "plt.hist(y,bins=np.arange(0.,100.,binwidth),density=True)\n",
    "plt.plot(np.arange(0.,100.,binwidth),lam*np.exp(-lam*np.arange(0.,100.,binwidth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be17eb3-f51f-4954-979d-d9d6feb4a3c1",
   "metadata": {},
   "source": [
    "You can get Gaussian random variables by the so-called Box-Muller transform. We'll demonstrate that it *works*, but for a thorough introduction read here: [Box-Muller Wikipedia](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c5258-b6e5-4144-8341-724309b7840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "N=10000\n",
    "\n",
    "def box_muller(u1, u2):\n",
    "    z0 = np.sqrt(-2*np.log(u1))*np.cos(2*np.pi*u2)\n",
    "    z1 = np.sqrt(-2*np.log(u1))*np.sin(2*np.pi*u2)\n",
    "    return z0, z1\n",
    "\n",
    "u1, u2 = np.random.rand(N), np.random.rand(N)\n",
    "\n",
    "z0, z1 = box_muller(u1, u2)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "axs[0].hist(u1,bins=30,density=True)\n",
    "axs[0].set_title('Uniform Distribution')\n",
    "axs[0].set_xlim([0,1])\n",
    "\n",
    "axs[1].hist(z0, bins=30, density=True, alpha=0.4, color='pink', edgecolor='black', label='z0')\n",
    "axs[1].hist(z1, bins=30, density=True, alpha=0.4, color='lightgreen', edgecolor='black', label='z1')\n",
    "axs[1].set_title('Normal Distribution')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76881983-f9ad-477d-8c23-a848bcb3fa2a",
   "metadata": {},
   "source": [
    "# von Neumann rejection sampling\n",
    "\n",
    "Rejection sampling was invented by von Neumann in 1951. It is a simple and general method for generating arbitrary distributions. The idea is to sample uniformly from a region known to enclose the distribution of interest, and to reject any samples that lie outside the region.\n",
    "\n",
    "To do rejection sampling, draw a plot with your probability distribution, and on the same graph, plot another curve $f(x)$ which has finite area and lies everywhere above your original distribution, that is for some constant $C$, $$P(x)\\le C f(x).$$ We will call $f(x)$ the “comparison function”. You should be able to draw numbers from the comparison function $f(x)$. Generate a random number $x_i$ from $f(x)$ and a number $u$ form an uniform distribution. If $$u\\le \\frac{P(x)}{Cf(x)},$$ we accept $x_i$ as a valid sample; otherwise, we reject it and draw new random numbers.  The fraction of points accepted/rejected will depend on the ratio between the two areas. The closer the comparison function $f(x)$ resembles $P(x)$, the more points will be accepted. Ideally, for $P(x)=f(x)$, all the points will be accepted, and none rejected. However, in practice, this is not always possible, but we can try to pick $f(x)$ such that we minimize the fraction of rejected points.\n",
    "\n",
    "No doubt you have seen rejection techniques in a common homework problem: computing $\\pi$ by drawing random points in a square between $0$ and $1$ and counting how many were inside a circle. The circle is the region enclosing the distribution of interest, the uniform density on the circle, and the square is the region from which you can easily draw, as shown above. By rejection sampling, you computed the volume of the circle relative to the volume of the square. That ratio is $\\pi/4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e2185-d4df-41bb-90db-d0a2405892d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def P(x):\n",
    "    \"\"\"Target distribution: Example - a custom probability density function (PDF).\"\"\"\n",
    "    return 0.3 * np.exp(-0.2 * x**2) + 0.7 * np.exp(-0.5 * (x - 2)**2)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Proposal distribution: A Gaussian with mean 2 and standard deviation 2.\"\"\"\n",
    "    return np.exp(-0.5 * ((x - 2) / 2) ** 2)\n",
    "\n",
    "# Constant C such that P(x) <= C * f(x) for all x\n",
    "C = 1\n",
    "\n",
    "def rejection_sampling(num_samples):\n",
    "    \"\"\"Von Neumann rejection sampling algorithm.\"\"\"\n",
    "    samples = []\n",
    "    inter = 0\n",
    "    while len(samples) < num_samples:\n",
    "        # Sample x from proposal distribution f(x)\n",
    "        x_candidate = np.random.normal(loc=2, scale=2)  # Sample from normal dist matching f(x)\n",
    "        u = np.random.uniform(0, 1)  # Uniform random number in [0,1]\n",
    "        inter=inter+1\n",
    "        # Acceptance criterion\n",
    "        if u < P(x_candidate) / (C * f(x_candidate)):\n",
    "            samples.append(x_candidate)\n",
    "        \n",
    "    return np.array(samples), inter\n",
    "\n",
    "# Generate samples\n",
    "num_samples = 10000\n",
    "samples, inter = rejection_sampling(num_samples)\n",
    "print(inter)\n",
    "# Plot results\n",
    "x_vals = np.linspace(-5, 7, 1000)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(samples, bins=50, density=True, alpha=0.6, label='Sampled Distribution')\n",
    "plt.plot(x_vals, P(x_vals), label='Target Distribution P(x)', linewidth=2)\n",
    "plt.plot(x_vals, C * f(x_vals), linestyle='dashed', label='Scaled Proposal C*f(x)', linewidth=2)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Von Neumann Rejection Sampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802fcaec-3ee6-45f8-878d-492e06eae2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "box_samples = np.random.rand(10000, 2)\n",
    "plt.scatter(box_samples[:,0], box_samples[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208aae81-7842-4b85-8116-237e78a76db6",
   "metadata": {},
   "source": [
    "# Random walk methods: the Metropolis algorithm\n",
    "\n",
    "Suppose that we want to generate random variables according to an arbitrary probability density $P(x)$. The Metropolis algorithm produces a “random walk” of points $\\{x_i\\}$ whose asymptotic probability approaches $P(x)$ after a large number of steps. The random walk is defined by a “transition probability” $w(x_i \\rightarrow x_j)$ for one value $x_i$ to another $x_j$ in order that the distribution of points $x_0$, $x_1$, $x_2$, ... converges to $P(x)$. In can be shown that it is sufficient (but not necessary) to satisfy the “detailed balance” condition \n",
    "$$p(x_i)w(x_i \\rightarrow x_j) = p(x_j)w(x_j \\rightarrow x_i).$$ \n",
    "This relation dos not specify $w(x_i \\rightarrow x_j)$ uniquely. A simple choice is \n",
    "$$w(x_i \\rightarrow x_j)=\\min{\\left[ 1,\\frac{P(x_j)}{P(x_i)} \\right] }.$$ \n",
    "This choice can be described by the following steps. Suppose that the “random walker” is a position $x_n$. To generate $x_{n+1}$ we\n",
    "\n",
    "1.  choose a trial position $x_t=x_n+\\delta _n$ , where the $\\delta _n$ is a random number in the interval $[-\\delta ,\\delta]$.\n",
    "\n",
    "2.  Calculate $w=P(x_t)/P(x_n)$.\n",
    "\n",
    "3.  If $w \\geq 1$ we accept the change and let $x_{n+1}=x_t$.\n",
    "\n",
    "4.  If $w \\leq 1$, generate a random number $r$.\n",
    "\n",
    "5.  If $r \\leq w$, accept the change and let $x_{n+1} = x_t$.\n",
    "\n",
    "6.  If the trial change is not accepted, the let $x_{n+1}=x_n$.\n",
    "\n",
    "It is necessary to sample a number of points of the random walk before the asymptotic probability $P(x)$ is attained. How do we choose the “step size” $\\delta$? If $\\delta$ is too large, only a small fraction of changes will be accepted and the sampling will be inefficient. If $\\delta$ is too small, a large number will be accepted, but it would take too long to sample $P(x)$ over the whole interval of interest. Ideally, we want at least 1/3-1/2 of the trial steps to be accepted. We also want to choose $x_0$ such that the distribution $\\{x_i\\}$ converges to $P(x)$ as quickly as possible. An obvious choice is to begin the random walk at the point where $P(x)$ is maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954c380-0aad-4bdd-807e-39f6bcf8274f",
   "metadata": {},
   "source": [
    "## The Gaussian distribution\n",
    "Let's use the Metropolis algorithm to generate a Gaussian distribution \n",
    "$$P(x)=A \\exp{(-x^2/2\\sigma ^2)}.$$ \n",
    "Is the numerical value of the normalization constant $A$ relevant? Can we determine a reasonable choice of $\\delta$ for a given $sigma$? (choose $x_0 = 0$.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b1bf7-518c-4423-bcc3-c03ec8e3d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(xold):\n",
    "    # Can we write a metropolis algorithm?\n",
    "    return xold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6032a-1220-421c-91ef-0f4c8d88e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "\n",
    "x = np.zeros(N)\n",
    "delta = 3.\n",
    "sigma = 20. \n",
    "sigma2 = sigma**2\n",
    "#initial sample of points\n",
    "xwalker = 20. \n",
    "Nwarmup = 5\n",
    "for i in range(Nwarmup):\n",
    "    xwalker = metropolis(xwalker)\n",
    "\n",
    "subsample = True\n",
    "\n",
    "x[0] = xwalker\n",
    "for i in range(1,N):\n",
    "    xold = x[i-1]\n",
    "    if not subsample:\n",
    "        xnew = metropolis(xold)\n",
    "    else:\n",
    "        for j in range(10):\n",
    "            xnew = metropolis(xold)\n",
    "            xold = xnew\n",
    "    x[i] = xnew\n",
    "\n",
    "binwidth = sigma/10\n",
    "bins = np.arange(-50,50,binwidth)\n",
    "plt.hist(x, bins=bins, density=True)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "norm = 1./(sigma*np.sqrt(2*np.pi))\n",
    "plt.plot(bins, norm*np.exp(-bins**2/(2*sigma2)), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e215f6e-ac1e-4a93-aa3b-ef09921949de",
   "metadata": {},
   "source": [
    "# Monte Carlo integration\n",
    "\n",
    "Imagine that we want to measure the area of a pond with arbitrary shape. Suppose that this pond is in the middle of a field with known area $A$. If we throw $N$ stones randomly, such that they land within the boundaries of the field, and we count the number of stones that fall in the pond $N_{in}$, the area of the pond will be approximately proportional to the fraction of stones that make a splash, multiplied by$A$: \n",
    "$$A_{pond}=\\frac{N_{in}}{N}A.$$ \n",
    "This simple procedure is an example of the “Monte Carlo” method.\n",
    "\n",
    "## Simple Monte Carlo integration\n",
    "\n",
    "More generaly, imagine a rectangle of height $H$ in the integration interval $[a,b]$, such that the function $f(x)$ is within its boundaries. Compute $n$ pairs of random numbers $(x_i,y_i)$ such that they are uniformly distributed inside this rectangle. The fraction of points that fall within the area contained below $f(x)$, <span>*i.e.*</span>, that satisfy $y_i \\leq f(x_i)$ is an estimate of the ratio of the integral of $f(x)$ and the area of the rectangle. Hence, the estimate of the integral will be given by:\n",
    "$$\\int _a^b{f(x)dx} \\simeq I(N) = \\frac{N_{in}}{N}H(b-a).$$\n",
    "\n",
    "Another Monte Carlo procedure is based on the definition:\n",
    "$$\\langle f \\rangle=\\frac{1}{(b-a)} \\int _a^b{f(x)dx}.$$ \n",
    "In order to determine this average, we sample the value of $f(x)$:\n",
    "$$\\langle f \\rangle \\simeq \\frac{1}{N}\\sum_{i=1}^{N}f(x_i),$$ \n",
    "where the $N$ values $x_i$ are distributed uniformly in the interval $[a,b]$. The\n",
    "integral will be given by $$I(N)=(b-a) \\langle f \\rangle.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18052a8e-a889-4b68-821d-9088c4332949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple integration\n",
    "samples = np.random.random(100000)\n",
    "x2 = samples**2\n",
    "my_integral = np.mean(x2) # this is the integral of x^2 from 0 to 1\n",
    "print(my_integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d83e1-81e0-4852-9e8f-9c2fc35719b8",
   "metadata": {},
   "source": [
    "## Monte Carlo error analysis\n",
    "\n",
    "The Monte Carlo method clearly yields approximate results. The accuracy depends on the number of values $N$ that we use for the average. A possible measure of the error is the “variance” $\\sigma^2$ defined by:\n",
    "$$\\sigma ^2=\\langle f^2 \\rangle - \\langle f \\rangle ^2,$$ \n",
    "where\n",
    "$$\\langle f \\rangle = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$$ \n",
    "and\n",
    "$$\\langle f^2 \\rangle = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)^2.$$ \n",
    "The “standard deviation” is $\\sigma$. However, we should expect that the error decreases with the number of points $N$, and the quantity $\\sigma$ defines above does not. Hence, this cannot be a good measure of the error.\n",
    "\n",
    "Imagine that we perform several measurements of the integral, each of them yielding a result $I_n$. These values have been obtained with different sequences of $N$ random numbers. According to the central limit theorem, these values whould be normally dstributed around a mean $\\langle I \\rangle$. Suppouse that we have a set of $M$ of such measurements ${I_n}$. A convenient measure of the differences of these measurements is the “standard deviation of the means” $\\sigma_M$:\n",
    "$$\\sigma_M ^2=\\langle I^2 \\rangle - \\langle I \\rangle ^2,$$ \n",
    "where\n",
    "$$\\langle I \\rangle = \\frac{1}{M} \\sum_{n=1}^M I_n$$ \n",
    "and\n",
    "$$\\langle I^2 \\rangle = \\frac{1}{M} \\sum_{n=1}^{M} I_n^2.$$ \n",
    "It can be proven that\n",
    "$$\\sigma_M \\approx \\sigma/\\sqrt{N}.$$ \n",
    "This relation becomes exact in the limit of a very large number of measurements. Note that this expression implies that the error decreases with the square root of the number of trials, meaning that if we want to reduce the error by a factor 10, we need 100 times more points for the average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d03f8-185d-451c-a15b-08f01591e90d",
   "metadata": {},
   "source": [
    "## Variance reduction\n",
    "\n",
    "If the function being integrated does not fluctuate too much in the interval of integration, and does not differ much from the average value, then the standard Monte Carlo mean-value method should work well with a reasonable number of points. Otherwise, we will find that the variance is very large, meaning that some points will make small contributions, while others will make large contributions to the integral. If this is the case, the algorithm will be very inefficient. The method can be improved by splitting the function $f(x)$ in two $f(x)=f_1(x)+f_2(x)$, such that the integral of $f_1(x)$ is known, and $f_2(x)$ as a small variance. The “variance reduction” technique, consists then in evaluating the integral of $f_2(x)$ to obtain:\n",
    "$$\\int _a^b{f(x)dx}=\\int _a^b {f_1(x)dx} + \\int _a^b{f_2(x)dx} = \\int_a^b{f_1(x)dx}+J.$$\n",
    "\n",
    "## Importance Sampling\n",
    "\n",
    "Imagine that we want to sample the function $f(x)=e^{-x^2}$ in the interval $[0,1]$. It is evident that most of our points will fall in the region where the value of $f(x)$ is very small, and therefore we will need a large number of values to achieve a decent accuracy. A way to improve the measurement by reducing the variance is obtained by “importance sampling”. As the name says, the idea is to sample the regions with larger contributions to the integral. For this goal, we introduce a probability distribution $P(x)$ normalized in the interval of integration $$\\int _a^b{P(x)dx} = 1.$$ Then, we can rewrite the integral of $f(x)$ as \n",
    "$$I=\\int _a^b{\\frac{f(x)}{P(x)}P(x)dx}$$ \n",
    "We can evaluate this integral, by sampling according to the probability distribution $P(x)$ and evaluating the sum\n",
    "$$I(N)=\\frac{1}{N} \\sum_{i=1}^N \\frac{f(x_i)}{P(x_i)}.$$ \n",
    "Note that for the uniform case $P(x)=1/(b-a)$, the expression reduces to the simple Monte Carlo integral.\n",
    "\n",
    "We are free to choose $P(x)$ now. We wish to do it in a way to reduce and minimize the variance of the integrand $f(x)/P(x)$. The way to to this is picking a $P(x)$ that mimics $f(x)$ where $f(x)$ is large. If we are able to determine an appropiate $P(x)$, the integrand will be slowly\n",
    "varying, and hence the variance will be reduced. Another consideration is that the generation of points according to the distribution $P(x)$\n",
    "should be a simple task. As an example, let us consider again the integral \n",
    "$$I=\\int _0^1 {e^{-x^2}dx}.$$ \n",
    "A reasonable choice for a weigh function is $P(x)=Ae^{-x}$, where $A$ is a normalization constant.\n",
    "\n",
    "Notice that for $P(x)=f(x)$ the variance is zero! This is known as the zero variance property. There is a catch, though: The probability function $P(x)$ needs to be normalized, implying that in reality, $P(x)=f(x)/\\int f(x)dx$, which assumes that we know in advance precisely the integral that we are trying to calculate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880aa0d1-7200-4a3c-bda5-0b1a389bd994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as sci\n",
    "def f(x):\n",
    "    return np.exp(-x**2)\n",
    "\n",
    "a, b = -20, 20\n",
    "# use the trapezoid rule\n",
    "def trapezoidal_rule(f, a, b, n=1000):\n",
    "    x = np.linspace(a, b, n)\n",
    "    y = f(x)\n",
    "    dx = (b-a) / (n-1)\n",
    "    return sci.trapezoid(y, dx = dx)\n",
    "\n",
    "for n in [2**k for k in range(1,10)]:\n",
    "    print(trapezoidal_rule(f, a, b, n=n))\n",
    "# See how it does\n",
    "trape = trapezoidal_rule(f, a, b, n=1000000)\n",
    "print(trape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be4f784-bc33-4c3a-953d-6e6f23232f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a0180-a9d2-4cba-9ca0-3663431f423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_uniform(f,a,b,n=1000):\n",
    "    samples = np.random.uniform(a,b,n)\n",
    "    avg_value = np.mean(f(samples))\n",
    "    return (b-a)*avg_value\n",
    "\n",
    "def importance_sampling(f, n=1000):\n",
    "    samples = np.random.normal(loc=0,scale=1,size=n)\n",
    "    weights = f(samples) / (1/np.sqrt(2*np.pi)*np.exp(-samples**2/2))\n",
    "    return np.mean(weights)\n",
    "    \n",
    "# trap_result = trapzoidal_rule(f, a, b)\n",
    "importance_result = importance_sampling(f, n=1000000)\n",
    "monte_carlo_result = monte_carlo_uniform(f, a, b, n=1000000)\n",
    "\n",
    "print('Trapezoid:', trape)\n",
    "print('Importance sampling:', importance_result)\n",
    "print('Monte Carlo:', monte_carlo_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da72c0-6671-4373-bf44-90299657c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp, mc = [], []\n",
    "ns = [10**k for k in range(3,8)]\n",
    "for n in ns:\n",
    "    imp.append(importance_sampling(f, n))\n",
    "    mc.append(monte_carlo_uniform(f, a, b, n))\n",
    "\n",
    "plt.scatter(ns, imp, label='Importance Sampling')\n",
    "plt.scatter(ns, mc, label='Monte Carlo Uniform')\n",
    "plt.axhline(truthbyn, color='red', label='Ground Truth')\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbec5ee-d2e1-41b3-83c0-bb9b623d9183",
   "metadata": {},
   "source": [
    "## Monte Carlo Integration in Quantum Field Theory\r\n",
    "\r\n",
    "In quantum field theory, Feynman diagrams represent interactions between particles, and their corresponding integrals describe the probability amplitudes of such interactions. These integrals often take the form of loop integrals, which involve integrating over the momenta of virtual particles in intermediate states.\r\n",
    "\r\n",
    "A basic example of a Feynman propagator integral in momentum space is:\r\n",
    "\r\n",
    "$$\r\n",
    "I = \\int d^4k \\frac{1}{(k^2 - m^2 + i\\epsilon)}\r\n",
    "$$\r\n",
    "\r\n",
    "where:\r\n",
    "- $ k $ is the four-momentum of a virtual particle,\r\n",
    "- $ m $ is its mass,\r\n",
    "- $ i\\epsilon $ ensures proper contour deformation in complex analysis.\r\n",
    "\r\n",
    "This type of integral is high-dimensional and does not have a simple closed-form solution. Traditional numerical integration methods struggle in such cases due to the curse of dimensionality, making Monte Carlo integration a powerful alternative.\r\n",
    "\r\n",
    "In this section, we apply Monte Carlo integration to approximate this 4D Feynman propagator integral. This technique is widely used in lattice QCD, particle physics simulations, and cross-section calculations in collider experiments.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a3928-2870-42b9-92da-b1440f5145e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad, romberg\n",
    "\n",
    "def monte_carlo_integration(f, dim, N=100000):\n",
    "    \"\"\"\n",
    "    Monte Carlo integration for a high-dimensional integral.\n",
    "    \n",
    "    Parameters:\n",
    "        f : function\n",
    "            Function to integrate\n",
    "        dim : int\n",
    "            Number of dimensions\n",
    "        N : int, optional\n",
    "            Number of Monte Carlo samples (default is 100000)\n",
    "    \n",
    "    Returns:\n",
    "        float: Approximated integral value\n",
    "    \"\"\"\n",
    "    # Generate random points in dim-dimensional space in range [-1,1]\n",
    "    points = np.random.uniform(-1, 1, (N, dim))\n",
    "    \n",
    "    # Evaluate function at those points\n",
    "    func_values = np.apply_along_axis(f, 1, points)\n",
    "    \n",
    "    # Compute Monte Carlo estimate\n",
    "    volume = (2**dim)  # Since limits are [-1,1]\n",
    "    integral_estimate = volume * np.mean(func_values)\n",
    "    \n",
    "    return integral_estimate\n",
    "\n",
    "# Define Feynman Diagram integral function\n",
    "def feynman_integrand(k):\n",
    "    \"\"\"\n",
    "    Example Feynman diagram integral in 4D:\n",
    "    \n",
    "    Integral of the form: \n",
    "    \\int d^4k \\frac{1}{(k^2 - m^2 + i\\epsilon)}\n",
    "    \n",
    "    Assuming m = 1, epsilon = 1e-6 for numerical stability\n",
    "    \"\"\"\n",
    "    m2 = 1.0  # Mass squared\n",
    "    epsilon = 1e-6  # Small imaginary part\n",
    "    k_squared = np.sum(k**2)  # Compute k^2\n",
    "    return 1.0 / (k_squared - m2 + epsilon)\n",
    "\n",
    "# Compute Feynman diagram integral in 4D using Monte Carlo\n",
    "result_feynman = monte_carlo_integration(feynman_integrand, dim=4, N=1000000)\n",
    "print(f\"Monte Carlo approximation of Feynman integral (4D): {result_feynman}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6c3b8-32c0-4d3a-afb9-75529f2b3022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dcbd4b-c6e6-45a3-84f8-2401087913ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ddcc2e-00d6-434d-be79-003b4fb7a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
