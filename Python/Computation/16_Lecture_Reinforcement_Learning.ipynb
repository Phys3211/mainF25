{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37244cd7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Today's topic, Reinforcement Learning, gets us close to what people might traditionally imagine when they hear \"artificial intelligence.\" You might not instinctively call a network that identifies handwritten digits \"intelligent\", but you might apply that word to a program that learns to play chess or a robot that figures out how to walk. Reinforcement Learning (RL) is a subfield of machine learning that tackles precisely these kinds of problems - where an agent learns to act intelligently through interaction with its environment.\n",
    "\n",
    "## Core Concepts\n",
    "Think of reinforcement learning like teaching a dog a trick - the dog (agent) interacts with its surroundings (environment), and through trial and error, it figures out which actions lead to good outcomes (treats).\n",
    "\n",
    "The essential components of RL:\n",
    "- an *agent* is exploring an *environment.*\n",
    "- at any given time, the agent perceives a *state* of the environment, $s\\in S$.\n",
    "- the agent can take *actions*, $a\\in A$, that influence the environment and move it to a new state.\n",
    "\n",
    "### Policy Function\n",
    "How does the agent decide what action to take? This is governed by a policy $\\pi: \\rightarrow A$, a function that maps states to actions. The policy might be *deterministic* or *stochastic*, conditioned on the state.\n",
    "\n",
    "As the agent follows its policy, it generates a trajectory through state space - a sequence of states and actions. This trajectory either continues indefinitely or ends in a terminal state, such as winning a games or reaching a physical goal.\n",
    "\n",
    "### Rewards and Return\n",
    "At each time step $t$, the agent receives a reward $r_t$, which is a number that tells it how \"good\" or \"bad\" the outcome of its action was. These rewards accumulate into a *return*:\n",
    "$$G_t = \\Sigma_0^{\\inf} \\gamma^k r_{t+k+1}.$$\n",
    "Here, $\\gamma \\in[0,1]$ is a *discount factor* that controls how much the agent values future rewards. A $gamma$ close to 0 makes the agent short-sighted; a $gamma$ close to 1 makes it value long-term planning.\n",
    "\n",
    "### Value Functions\n",
    "To help the agent evaluate decisions, we define:\n",
    "- *state value function(s)* - $v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s]$. This is the expected return if the agent starts in state $s$ and follows policy $\\pi$\n",
    "- *action value function(s)* - $q_{\\pi}(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a]$. This tells us how good it is to take action $a$ in state $s$, assuming we follow policy $\\pi$ thereafter. \n",
    "\n",
    "## The Goal of RL\n",
    "The ultimate goal is to find a policy $\\pi$ that maximizes expected return over time. Two major methods are used:\n",
    "- Value-based methods: First estimate $v_{\\pi}(s)$ or $q_\\pi(s,a)$, then derive the policy from it.\n",
    "- Policy-based methods: Directly optimize the policy to maximize expected return. these are often called policy gradient methods.\n",
    "\n",
    "## When to use RL\n",
    "To decide whether reinforcment learning is the right approach for your problem, you should ask: \n",
    "- Is there a natural framing in terms of an agent exploring an environment?\n",
    "- Can the agent take discrete or continuous actions that affect outcomes?\n",
    "- Is there a clear notion of reward? What counts as good or bad?\n",
    "If so, RL may be appropriate\n",
    "\n",
    "In some simple environments (like in small grids or short games), you can exhaustively try all possible states and actions. But in complex environments with huge state spaces (think Chess, Go, or String Theory parameter landscapes), brute force becomes unrealistic. In those cases, we use function approximation - usually using neural networks - to estimate value functions or policies. This gives rise to the modern and powerful field of *deep reinforcement learning*\n",
    "\n",
    "For more on RL, see:\n",
    "- [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html), a famous early textbook.\n",
    "- [David Silver's course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), a more modern take. The videos were my entry point into RL.\n",
    "- [AlphaZero](https://www.nature.com/articles/nature24270), a 2017 breakthrough in which RL achieves superhuman gameplay in Go *without human knowledge*, i.e. only via knowledge of the game and self-play. It was extended to Chess and Shogi in 2018. See the arXiv article here: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815).\n",
    "- My friends and I introduced RL into string theory in [Branes with Brains](https://arxiv.org/abs/1903.11616). We have also used it to [find unknots](https://arxiv.org/abs/2010.16263) and [ribbons](https://arxiv.org/abs/2304.09304), the latter in connection with the smooth 4d Poincaré conjecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138c7ad",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "In this class we'll study a famous game in RL called *Gridworld*, see. e.g. Sutton and Barto for more. It's a simple game that is easy to understand, but still has some interesting features. The game is played on a grid, and the agent can move up, down, left, or right. The agent starts in a random position, and the goal is to reach the goal state, which is chosen via the policy.\n",
    "The game ends when the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c19aaa",
   "metadata": {},
   "source": [
    "First we'll set up some helper functions that will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# get the best action. If several actions are equally good, choose a random one\n",
    "def get_best_action(dct):\n",
    "    best_actions = []\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in dct.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "            best_actions = [[max_key, max_val]]\n",
    "        elif v == max_val:\n",
    "            best_actions.append([k, v])\n",
    "\n",
    "    return best_actions[np.random.randint(0, len(best_actions))]\n",
    "\n",
    "# randomize the action in 100*eps percent of the cases\n",
    "def random_action(action, action_space, eps=0.3):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return action\n",
    "    else:\n",
    "        return np.random.choice(action_space)\n",
    "\n",
    "# Animate the steps taken, ignore this if you don't care\n",
    "step_counter = 0\n",
    "explore_step = None\n",
    "def animate_steps(agent, window_title, fig_title=\"\"):\n",
    "    plt.ioff()\n",
    "    fig = plt.figure(window_title)\n",
    "    fig.suptitle(fig_title)\n",
    "    mySteps = agent.steps_taken\n",
    "    agent.reset()\n",
    "    step_counter = 0\n",
    "    explore_step = mySteps[step_counter]\n",
    "    im = plt.imshow(agent.render_world(), animated=True)\n",
    "\n",
    "    def update_fig(*args):\n",
    "        nonlocal explore_step, step_counter\n",
    "        if step_counter < len(mySteps):\n",
    "            explore_step = mySteps[step_counter]\n",
    "        else:\n",
    "            step_counter = 0\n",
    "            explore_step = mySteps[step_counter]\n",
    "            agent.reset()\n",
    "        agent.step(explore_step, False)\n",
    "        step_counter += 1\n",
    "        im.set_array(agent.render_world())\n",
    "        plt.draw()\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_fig, interval=150, blit=True, frames=len(mySteps)-1, repeat=True)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(window_title)\n",
    "    # plt.show()\n",
    "    ani.save(\"videos/\"+window_title + \".gif\", writer=animation.PillowWriter(fps=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36fc3c",
   "metadata": {},
   "source": [
    "## Defining Gridworld\n",
    "This module defines the Gridworld game environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aabb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "\n",
    "# The gridworld environment\n",
    "class GameEnv:\n",
    "    def __init__(self):\n",
    "        # initialization of the world\n",
    "        self.sizeX = 12\n",
    "        self.sizeY = 12\n",
    "        self.state = ()  # A state in gridworld is just the (x,y) coordinate pair of the worker\n",
    "        plt.ioff()  # there is currently a bug for Mac users which requires turning this off (as of Fall '24)\n",
    "        self.world_canvas = plt.figure(\"Gridworld\")\n",
    "        self.world_canvas.suptitle('Blue: Worker, Red: Pitfalls, Green: Exit')\n",
    "        self.im = None\n",
    "        plt.axis(\"off\")\n",
    "        self.objects = []\n",
    "        self.initial_x = 0\n",
    "        self.initial_y = 0\n",
    "        self.gave_up = False\n",
    "\n",
    "        # We want the worker to solve the maze as fast as possible without falling into the pits:\n",
    "        # *)   -1 for each step (penalty to solve it quickly)\n",
    "        # *)  -50 for each pitfall (penalty for falling into the pit)\n",
    "        # *) +100 for finding the exit (reward for solving the maze)\n",
    "        # *)   -2 for running into a wall / not moving at all\n",
    "        self.step_penalty = -1.\n",
    "        self.pitfall_penalty = -50.\n",
    "        self.exit_reward = 100.\n",
    "        self.no_move_penalty = -2.\n",
    "\n",
    "        # Actions in gridworld: move up, down, left, right\n",
    "        self.action_space = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "        # keep track of the total number of steps and the steps that were taken in the game\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "\n",
    "        # maximal number of steps before we give up solving the maze\n",
    "        self.max_steps = 100\n",
    "\n",
    "        # initialize and plot the world\n",
    "        self.world = self.initialize_world()\n",
    "\n",
    "    # initialize a new random world\n",
    "    def initialize_world(self):\n",
    "        self.objects = []\n",
    "\n",
    "        # 1.) The first parameter is the name of the object\n",
    "        # 2.) The second parameter is the reward / penalty:\n",
    "        # 3.) The third parameter is the position of the object in the world\n",
    "        # 4.) Ignore the other parameters, they are just used for drawing the world (box sizes and color)\n",
    "        maze_exit = GameOb('exit', self.exit_reward, self.new_position(), 1, [0, 1, 0, 1])\n",
    "        self.objects.append(maze_exit)\n",
    "        worker = GameOb('worker', None, self.new_position(), 1, [0, 0, 1, 1])\n",
    "        self.objects.append(worker)\n",
    "        for i in range(6):  # add pitfalls\n",
    "            pitfall = GameOb('pitfall', self.pitfall_penalty, self.new_position(), 1, [1, 0, 0, 1])\n",
    "            self.objects.append(pitfall)\n",
    "\n",
    "        # store the initial (x,y) coordinates for a reset\n",
    "        self.initial_x = worker.x\n",
    "        self.initial_y = worker.y\n",
    "\n",
    "        # show the world\n",
    "        world = self.render_world()\n",
    "\n",
    "        # initialize/reset the variables\n",
    "        self.reset()\n",
    "\n",
    "        # plot the world\n",
    "        plt.ioff()\n",
    "        self.im = plt.imshow(world, interpolation=\"nearest\")\n",
    "\n",
    "        return world\n",
    "\n",
    "    # reset the world to its initial configuration, ignore this\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "        self.gave_up = False\n",
    "        self.state = (self.initial_x, self.initial_y)\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                obj.x = self.initial_x\n",
    "                obj.y = self.initial_y\n",
    "                break\n",
    "\n",
    "    # move through the world\n",
    "    # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "    def move_worker(self, direction):\n",
    "\n",
    "        # identify the worker amongst the gridworld objects\n",
    "        worker = None\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                worker = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "\n",
    "        worker_x = worker.x\n",
    "        worker_y = worker.y\n",
    "\n",
    "        # overall reward/penalty\n",
    "        reward = self.step_penalty  # penalize each move\n",
    "\n",
    "        # update the position of the worker in gridworld (move if possible)\n",
    "        if direction == 0 and worker.y >= 1:\n",
    "            worker.y -= 1\n",
    "        if direction == 1 and worker.y <= self.sizeY - 2:\n",
    "            worker.y += 1\n",
    "        if direction == 2 and worker.x >= 1:\n",
    "            worker.x -= 1\n",
    "        if direction == 3 and worker.x <= self.sizeX - 2:\n",
    "            worker.x += 1\n",
    "\n",
    "        # move was illegal\n",
    "        if worker.x == worker_x and worker.y == worker_y:\n",
    "            reward = self.no_move_penalty\n",
    "\n",
    "        # update to new position\n",
    "        for i in range(len(self.objects)):\n",
    "            if self.objects[i].name == 'worker':\n",
    "                self.objects[i] = worker\n",
    "                break\n",
    "\n",
    "        # check whether new field is a special field (exit/pitfall) and compute reward/penalty\n",
    "        is_maze_solved = False\n",
    "        for other in others:\n",
    "            if worker.x == other.x and worker.y == other.y:  # the worker ran into an object\n",
    "                if other.name == \"exit\":  # the object was an exit\n",
    "                    is_maze_solved = True\n",
    "                    reward = other.reward\n",
    "                    break  # we can exit the loop here since we can only run into one object\n",
    "                elif other.name == \"pitfall\":  # the object was a pitfall\n",
    "                    is_maze_solved = False\n",
    "                    reward = other.reward\n",
    "                    break   # we can exit the loop here since we can only run into one object\n",
    "\n",
    "        return reward, is_maze_solved\n",
    "\n",
    "    # perform the step, collect the reward, check whether you have reached the exit\n",
    "    def step(self, action, update_view=True):\n",
    "\n",
    "        # collect the reward/punishment for the field the worker ends up in and check whether the exit was reached\n",
    "        reward, done = self.move_worker(action)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.steps_taken.append(action)\n",
    "\n",
    "        # give up\n",
    "        if self.steps >= self.max_steps and not done:\n",
    "            done = True\n",
    "            self.gave_up = True\n",
    "\n",
    "        # this just updates the graphic output of the world\n",
    "        if update_view:\n",
    "            self.im.set_array(self.render_world())\n",
    "            plt.draw()\n",
    "\n",
    "        # return the new state, the penalty/reward for the move and whether gridworld is solved/given up on\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    # get the current state\n",
    "    def get_state(self):\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                return (obj.x, obj.y)\n",
    "\n",
    "    # check whether an action is possible, i.e. whether a wall is blocking the way\n",
    "    def is_possible_action(self, action):\n",
    "        is_possible = False\n",
    "        if action == 0 and self.state[1] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 1 and self.state[1] <= self.sizeY - 2:\n",
    "            is_possible = True\n",
    "        if action == 2 and self.state[0] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 3 and self.state[0] <= self.sizeX - 2:\n",
    "            is_possible = True\n",
    "\n",
    "        return is_possible\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # ignore the code from here on, it just draws the world and represents the objects in the game.\n",
    "    ####################################################################################################################\n",
    "    def close_world_display(self):\n",
    "        plt.close(\"Gridworld\")\n",
    "\n",
    "    def new_position(self):\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        current_position = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in current_position:\n",
    "                current_position.append((objectA.x, objectA.y))\n",
    "        for pos in current_position:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def render_world(self):\n",
    "        a = np.zeros([self.sizeY + 2, self.sizeX + 2, 4])\n",
    "        a[0:, 0, 3] = 1  # left wall\n",
    "        a[0, 0:, 3] = 1  # top wall\n",
    "        a[0:, self.sizeX + 1, 3] = 1  # right wall\n",
    "        a[self.sizeY + 1, 0:, 3] = 1  # bottom wall\n",
    "        a[1:-1, 1:-1, :] = 1\n",
    "        for item in self.objects:\n",
    "            if a[item.y + 1, item.x + 1, 0] == 1 and a[item.y + 1, item.x + 1, 1] == 1 and a[item.y + 1, item.x + 1, 2] == 1:  # is completely white\n",
    "                for i in range(len(item.channel)): a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] = item.channel[i]\n",
    "            else:  # other object on the field, overlay worker with pitfalls / exit\n",
    "                for i in range(len(item.channel)):\n",
    "                    if a[item.y + 1, item.x + 1, i] == 0:\n",
    "                        a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] += item.channel[i]\n",
    "        a = scipy.ndimage.zoom(a[:, :], (4, 4, 1), order=0, mode=\"nearest\")\n",
    "        return a\n",
    "\n",
    "# This represents an object in the game: worker, pitfall, exit\n",
    "class GameOb:\n",
    "    def __init__(self, name, reward, coordinates, size, RGBA):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.channel = RGBA\n",
    "        self.reward = reward\n",
    "        self.name = name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e76f",
   "metadata": {},
   "source": [
    "Now we have the agent explore Gridworld. Since the state space is small, we can explore the entire environment and find optimal policies. We will do so using an algorithm called *SARSA*. The basic idea is to estimate the action-value function $q_\\pi(s,a)$, and then use it to find the optimal policy. The algorithm is as follows:\n",
    "Initialize $Q(s,a)$ arbitrarily.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81528845-fca1-4987-a673-fe501d6b6628",
   "metadata": {},
   "source": [
    "## Implementing Gridworld\n",
    "Based on a SARSA implementation downloaded at [sarsa.py](https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/sarsa.py). It was further modified by Fabian Ruehle. Below, we initialize the environment, train the agent, and solve the puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d34cbf-1d4d-4d39-a120-7c63ce6662e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on a SARSA implementation downloaded from \n",
    "# Modified and extended by Fabian Ruehle\n",
    "\n",
    "# Explore Gridworld\n",
    "# import gridworld\n",
    "# import helperFunctions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gridworld:\n",
    "# *) Worker:    Blue\n",
    "# *) Pitfalls:  Red\n",
    "# *) Exit:      Green\n",
    "agent = GameEnv()\n",
    "\n",
    "agent.step(0)\n",
    "print(\"I moved up\")\n",
    "agent.step(1)\n",
    "print(\"I moved down\")\n",
    "agent.step(2)\n",
    "print(\"I moved left\")\n",
    "agent.step(3)\n",
    "print(\"I moved right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2350f-cbc0-41b8-9ca5-a6b2e72d541f",
   "metadata": {},
   "source": [
    "### Part 1: Play the game once to see an untrained agent at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02635bd-478a-43b1-a5f1-cf20007524c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.reset()\n",
    "agent.close_world_display()\n",
    "print(\"Let the game begin...\")\n",
    "\n",
    "# generate all states\n",
    "all_states = []\n",
    "for x in range(agent.sizeX):\n",
    "    for y in range(agent.sizeY):\n",
    "        all_states.append((x, y))\n",
    "\n",
    "# Q is a dictionary that contains the rewards for all four actions that can be performed in any given square of Gridworld.\n",
    "# Initialize Q and keep track of how many times Q[s] has been updated\n",
    "Q = {}\n",
    "update_counts_sa = {}\n",
    "for s in all_states:\n",
    "    update_counts_sa[s] = {}\n",
    "    Q[s] = {}\n",
    "    for a in agent.action_space:\n",
    "        update_counts_sa[s][a] = 1.0\n",
    "        Q[s][a] = 0.0\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "alpha_W = 0.1  # learning rate\n",
    "t = 1.0  # count time\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# To start the algorithm, we need any action, so we pick one randomly until we find a valid action which we perform\n",
    "found_initial_move = False\n",
    "current_action = None\n",
    "current_state = agent.get_state()\n",
    "while not found_initial_move:\n",
    "    current_action = random_action(None, agent.action_space, eps=1)\n",
    "    found_initial_move = agent.is_possible_action(current_action)\n",
    "\n",
    "episode_reward = 0\n",
    "q_update_log = []\n",
    "# loop until done (i.e. solved the maze or gave up)\n",
    "done = False\n",
    "while not done:\n",
    "    # perform the current step and get the next state, the reward/penalty for the move, and whether the agent is done (solved or gave up)\n",
    "    next_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "    # get the best currently known action for the state we are in now\n",
    "    next_action = get_best_action(Q[current_state])[0]\n",
    "    # randomize the action to allow for exploration. As time progresses, make random actions less likely.\n",
    "    next_action = random_action(next_action, agent.action_space, eps=0.4/t)\n",
    "    # Log pre-update Q-value\n",
    "    old_q = Q[current_state][current_action]\n",
    "    # Update Q\n",
    "    alpha = alpha_W / update_counts_sa[current_state][current_action]\n",
    "    update_counts_sa[current_state][current_action] += 0.005\n",
    "    Q[current_state][current_action] = Q[current_state][current_action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[current_state][current_action])\n",
    "    #Log info\n",
    "    new_q = Q[current_state][current_action]\n",
    "    q_update_log.append((current_state, current_action, old_q, reward, new_q))\n",
    "    episode_reward += reward\n",
    "    # update current state, current action, and start over\n",
    "    current_state = next_state\n",
    "    current_action = next_action\n",
    "    t += 0.001\n",
    "\n",
    "print(f\"Episode ended. Total reward: {episode_reward:.2f}\")\n",
    "print(\"Sample Q-updates (state, action, old Q, reward, new Q):\")\n",
    "for i, (s, a, old_q, r, new_q) in enumerate(q_update_log[:5]):  # limit to 5 for clarity\n",
    "    print(f\"  {i+1}: {s}, {a} → Q: {old_q:.2f} → {new_q:.2f} with reward {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633f0aa-1830-40f2-a92c-541159f167b6",
   "metadata": {},
   "source": [
    "### Part 2: Show the exploration route taken by the untrained worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dc874-40bf-4cfb-9df1-0fef07f68c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show exploration route\n",
    "result = \"\"\n",
    "if not agent.gave_up:\n",
    "    result = \"I solved gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"Sorry, I had to give up after \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the first game\n",
    "print(\"Watch my exploration route... (close the plot window to continue)\")\n",
    "animate_steps(agent, \"Gridworld exploration untrained worker\", result)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url='videos/Gridworld exploration untrained worker.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5854c31-c3a3-49bf-bfe4-b0d2094b390e",
   "metadata": {},
   "source": [
    "### Part 3: Play the game 10,000 time to learn the best solution strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f5a7e-48a3-4fa1-b1fa-51c9c8377d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now let me train for a while, I enjoyed the game so much!\")\n",
    "steps_per_episode = []\n",
    "\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "# The code is essentially identical to the one used above, but now carried out 10,000 times\n",
    "training_episodes = 10000\n",
    "for i in range(training_episodes):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"I'm playing game \" + str(i) + \" / \" + str(training_episodes))\n",
    "    if i % 100 == 0:\n",
    "        t += 0.01\n",
    "    agent.reset()\n",
    "    episode_steps = 0\n",
    "    found_initial_move = False\n",
    "    current_action = None\n",
    "    current_state = agent.get_state()\n",
    "    while not found_initial_move:\n",
    "        current_action = random_action(None, agent.action_space, eps=1)\n",
    "        found_initial_move = agent.is_possible_action(current_action)\n",
    "    done = False\n",
    "\n",
    "    \n",
    "    # loop until done (i.e. solved the maze or gave up)\n",
    "    while not done:\n",
    "        # perform the current step and get the next state, the reward/penalty for the move, and whether the agent is done (solved or gave up)\n",
    "        next_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "        # get the best currently known action for the state we are in now\n",
    "        next_action = get_best_action(Q[current_state])[0]\n",
    "        # randomize the action to allow for exploration. As time progresses, make random actions less likely.\n",
    "        next_action = random_action(next_action, agent.action_space, eps=0.4/t)\n",
    "\n",
    "        # Update Q\n",
    "        alpha = alpha_W / update_counts_sa[current_state][current_action]\n",
    "        update_counts_sa[current_state][current_action] += 0.005\n",
    "        Q[current_state][current_action] = Q[current_state][current_action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[current_state][current_action])\n",
    "\n",
    "        # update current state, current action, and start over\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "        episode_steps += 1\n",
    "    steps_per_episode.append(episode_steps)\n",
    "\n",
    "\n",
    "print(\"Ok, I am done practicing.\")\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps_per_episode[9900:],'bo')\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps to Goal\")\n",
    "plt.title(\"Learning Curve: Steps per Episode\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57147a7c-16c9-41bd-9215-b3d39d9b51d9",
   "metadata": {},
   "source": [
    "### Part 4: Show the exploration route taken by the trained worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c851e8-59e0-4c1b-bf5e-e8adf6d1ab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Part 4: Show the exploration route taken by the trained worker\n",
    "########################################################################################################################\n",
    "\n",
    "# Navigate the maze using the best steps as learned by the agent\n",
    "current_state = agent.get_state()\n",
    "done = False\n",
    "while not done:\n",
    "    current_action = get_best_action(Q[current_state])[0]\n",
    "    current_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "result = \"\"\n",
    "if not agent.gave_up:\n",
    "    result = \"I can now solve Gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"I haven't learned solving Gridworld in \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the trained worker\n",
    "print(\"Watch my exploration route... (close the plot window to continue)\")\n",
    "animate_steps(agent, \"Gridworld exploration trained worker\", result)\n",
    "\n",
    "print(\"Thanks for playing! Bye.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d14d7-1caf-47c2-9cc9-43c23129a816",
   "metadata": {},
   "source": [
    "## Finally: Show the resulting gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e2b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='videos/Gridworld exploration trained worker.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95daea0-eb8d-4eca-ac47-bb6fffced5f2",
   "metadata": {},
   "source": [
    "You can see the $Q$ values found by the agent by looking at a heatmap. Why is the goalhave a low $Q$-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b7df1-5a2a-4852-b385-e98297e27948",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_map = np.zeros((agent.sizeY, agent.sizeX))\n",
    "for x in range(agent.sizeX):\n",
    "    for y in range(agent.sizeY):\n",
    "        state = (x, y)\n",
    "        best_q = max(Q[state].values())\n",
    "        value_map[y, x] = best_q  # note: row = y, col = x\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(value_map, cmap='viridis')\n",
    "plt.colorbar(label='Max Q-value')\n",
    "plt.title('Q-Value Heatmap')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d151aa4-0b9b-4b45-9579-8dde2fd619d2",
   "metadata": {},
   "source": [
    "## Try to extract the best move per state from Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ff45e-1a64-4235-9654-cbc3b89496b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "arrow_dict = {0: (0, -1), 1: (0, 1), 2: (-1, 0), 3: (1, 0)}  # up, down, left, right\n",
    "\n",
    "U = np.zeros((agent.sizeY, agent.sizeX))\n",
    "V = np.zeros((agent.sizeY, agent.sizeX))\n",
    "\n",
    "for x in range(agent.sizeX):\n",
    "    for y in range(agent.sizeY):\n",
    "        state = (x, y)\n",
    "        best_action = get_best_action(Q[state])[0]\n",
    "        dx, dy = arrow_dict[best_action]\n",
    "        U[y, x] = dx\n",
    "        V[y, x] = -dy  # minus because y=0 is top in matplotlib\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.quiver(U, V, pivot='middle')\n",
    "plt.title(\"Learned Policy (Best Actions per State)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c5a54-b67c-45e7-ab3f-757f4352df2b",
   "metadata": {},
   "source": [
    "# Problem X: Shaping Random Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac35e7-0ba6-47d1-956b-108b1718547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning for Shaping Random Points\n",
    "\n",
    "# Step 1: Imports and Setup\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Step 2: Define the Policy Network\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # Output delta x and delta y\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Step 3: Define the Environment and Reward Function\n",
    "# We use the unit circle as the target shape.\n",
    "\n",
    "def reward_circle(point):\n",
    "    r = torch.norm(point, dim=1)\n",
    "    return -((r - 1.0)**2)  # Max reward at r=1\n",
    "\n",
    "def reward_square(point):\n",
    "    x = point[:, 0]\n",
    "    y = point[:, 1]\n",
    "\n",
    "    # Distance to each edge\n",
    "    dist_top = (y - 1).abs()\n",
    "    dist_bottom = (y + 1).abs()\n",
    "    dist_left = (x + 1).abs()\n",
    "    dist_right = (x - 1).abs()\n",
    "\n",
    "    # Penalize distance to the *closest* edge\n",
    "    edge_dist = torch.min(torch.stack([dist_top, dist_bottom, dist_left, dist_right], dim=1), dim=1).values\n",
    "\n",
    "    # Also penalize if outside bounds (keep points close to the square)\n",
    "    out_of_bounds = torch.relu(x.abs() - 1) + torch.relu(y.abs() - 1)\n",
    "\n",
    "    return -(edge_dist + out_of_bounds)  # Negative = loss, reward is highest at edge\n",
    "def reward_swiss_cross(point):\n",
    "    x = point[:, 0]\n",
    "    y = point[:, 1]\n",
    "\n",
    "    bar_width = 0.2\n",
    "\n",
    "    # Penalize distance to the horizontal and vertical bars\n",
    "    dist_to_vert = torch.relu(x.abs() - bar_width)\n",
    "    dist_to_horiz = torch.relu(y.abs() - bar_width)\n",
    "\n",
    "    # Reward for being close to either bar (closer = better)\n",
    "    dist = torch.min(dist_to_vert, dist_to_horiz)\n",
    "\n",
    "    # Also encourage being within bounds\n",
    "    radius_penalty = 0.05 * (x**2 + y**2)\n",
    "\n",
    "    return -(dist + radius_penalty)\n",
    "\n",
    "# Step 4: Training Loop (REINFORCE-style)\n",
    "\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "num_episodes = 3000\n",
    "batch_size = 128\n",
    "loss_history = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Sample random 2D points (state) and their mirror images\n",
    "    half = batch_size // 2\n",
    "    half_state = torch.randn(half, 2)\n",
    "    state = torch.cat([half_state, -half_state], dim=0)\n",
    "\n",
    "    # Predict action: a displacement vector\n",
    "    action = policy(state)\n",
    "    \n",
    "    # Apply action\n",
    "    moved = state + action\n",
    "\n",
    "    # Get reward\n",
    "    reward = reward_circle(moved)\n",
    "\n",
    "    # Compute loss (REINFORCE trick)\n",
    "    loss = -reward.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 5: Visualize the Learned Shape\n",
    "\n",
    "# Generate and transform test points\n",
    "with torch.no_grad():\n",
    "    test_points = torch.randn(1000, 2)\n",
    "    transformed = test_points + policy(test_points)\n",
    "    \n",
    "    transformed = transformed.numpy()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(transformed[:, 0], transformed[:, 1], alpha=0.5, edgecolor='k')\n",
    "plt.title(\"Points Transformed Toward Unit Circle\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Plot Training Loss\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Loss Over Time\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss (negative reward)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ec462-4fcb-43b0-8da4-b63642d180eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
