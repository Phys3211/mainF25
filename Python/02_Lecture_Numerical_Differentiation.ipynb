{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Differentiation\n",
    "========================\n",
    "Numeric differentiation is a technique used to approximate the derivative of a function when an analytical derivative is difficult or impossible to obtain. It is widely used to analyze rates of change in discrete data sets or complex functions. The most common methods include finite difference approximations, such as **forward**, **backward**, and **central** differences, which estimate derivatives using function values at nearby points. While numerical differentiation is straightforward to implement, it can introduce errors due to finite precision and step size selection, requiring careful consideration of accuracy and stability.\n",
    "\n",
    "## Forward/Backward Difference \n",
    "\n",
    "The forward/backward difference uses the traditional equation for differentiation:\n",
    "\n",
    "$$\\frac{dy}{dx} = y'(x) =  \\frac{y(x+\\Delta x) - y(x)}{h}$$\n",
    "\n",
    "where $h$ is the **step size**, also denoted as $dx\\approx\\Delta x$.\n",
    "\n",
    "In order to numerically evaluate a derivative $y'(x)=dy/dx$ at point $x_0$, we approximate is by using finite differences.\n",
    "Therefore we find: \n",
    "\n",
    "$dx \\approx \\Delta x =x_1-x_0 = h$\n",
    "\n",
    "$dy \\approx \\Delta y =y_1-y_0= y(x_1)-y(x_0) = y(x_0+\\Delta x)-y(x_0)$\n",
    "\n",
    "Then we re-write the derivative in terms of discrete differences as:\n",
    "$$\\frac{dy}{dx} \\approx \\frac{\\Delta y}{\\Delta x}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's look at the accuracy of this approximation in terms of the interval $\\Delta x$. In our first example we will evaluate the derivative of $y=x^2$ at $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "step = []\n",
    "er = []\n",
    "while(dx > 1.e-10):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step.append(dx)\n",
    "    er.append(d-2)\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 10.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# need absolute values for log plot\n",
    "ers = [abs(x) for x in er ]\n",
    "plt.plot(step,ers)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it that the sequence does not converge? This is due to the round-off errors in the representation of the floating point numbers. To see this, we can simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((1.+0.0001)*(1+0.0001)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using powers of 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "step2 = []\n",
    "er2 = []\n",
    "while(dx > 1.e-10):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step2.append(dx)\n",
    "    er2.append(d-2)\n",
    "    print(\"%8.5e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need absolute values for log plot\n",
    "ers2 = [abs(x) for x in er2 ]\n",
    "plt.plot(step2,ers2)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appeared to have less trouble as the step size got smaller. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central (Midpoint) Difference\n",
    "\n",
    "The central difference method is a more accurate numerical differentiation technique compared to forward or backward differences because it utilizes points on both sides of the target point to estimate the derivative. It is given by the formula:\n",
    "$$ \\frac{dy}{dx} \\approx \\frac{y(x_0+\\frac{h}{2})-y(x_0-\\frac{h}{2})}{h}.$$\n",
    "By averaging symmetric function values around $x$, the central difference method reduces truncation error to the order of $\\mathcal{O}(h^2)$\n",
    "\n",
    "\n",
    "For a more complex function we may need to import it from the math module. For instance, let's calculate the derivative of $sin(x)$ at $x=\\pi/4$, including both the forward and central differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sin, sqrt, pi\n",
    "dx = 1.\n",
    "data = []\n",
    "while(dx > 1.e-16):\n",
    "    x = pi/4.\n",
    "    d1 = sin(x+dx) - sin(x); #forward\n",
    "    d2 = sin(x+dx*0.5) - sin(x-dx*0.5); # midpoint\n",
    "    d1 = d1 / dx;\n",
    "    d2 = d2 / dx;\n",
    "    e1 = d1-sqrt(2.)/2.\n",
    "    e2 = d2-sqrt(2.)/2.\n",
    "    print(\"%8.5e %20.16f %20.16f %20.16f %20.16f\" % (dx, d1, e1, d2, e2) )\n",
    "    data.append([dx,d1,e1,d2,e2])\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? Which one does better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(arraydata[:,0],abs(arraydata[:,2]),'r--')\n",
    "plt.plot(arraydata[:,0],abs(arraydata[:,4]),'b')\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more in-depth discussion about round-off errors in numerical differentiation can be found <a href=\"http://www.uio.no/studier/emner/matnat/math/MAT-INF1100/h10/kompendiet/kap11.pdf\">here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special functions in **numpy**\n",
    "\n",
    "numpy provides a simple method **diff()** to calculate the numerical derivatives of a dataset stored in an array by forward differences. The function **gradient()** will calculate the derivatives by midpoint (or central) difference, that provides a more accurate result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y = lambda x: x*x\n",
    "\n",
    "x1 = np.arange(0,10,1)\n",
    "x2 = np.arange(0,10,0.1)\n",
    "\n",
    "y1 = np.gradient(y(x1), 1.)\n",
    "print(y1)\n",
    "\n",
    "pyplot.plot(x1,np.gradient(y(x1),1.),'r--o');\n",
    "pyplot.plot(x1[:x1.size-1],np.diff(y(x1))/np.diff(x1),'b--x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that **gradient()** uses forward and backward differences at the two ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x2,np.gradient(y(x2),0.1),'b--o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More discussion about numerical differentiation, including higher order methods with error extrapolation can be found <a href=\"http://young.physics.ucsc.edu/115/diff.pdf\">here</a>. \n",
    "\n",
    "The module **scipy** also includes methods to accurately calculate derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import derivative\n",
    "\n",
    "y = lambda x: x**2 # difference between x^3 and x^2\n",
    "\n",
    "dx = 1.\n",
    "x = 1.\n",
    "\n",
    "while(dx > 1.e-10):\n",
    "    d = derivative(y, x, dx, n=1, order=3)\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you may see, `derivative` seems to be deprecated, and `gradient` seems to be preferred now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D array representing your function\n",
    "f = np.array([[1, 2, 6], [3, 4, 5]])\n",
    "\n",
    "# Calculate the gradient\n",
    "gradient = np.gradient(f)\n",
    "\n",
    "# gradient is a tuple of two arrays: \n",
    "# - gradient[0]: gradient along the rows (y-axis)\n",
    "# - gradient[1]: gradient along the columns (x-axis)\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid size\n",
    "x = np.linspace(-2, 2, 20)  # 20 points from -2 to 2\n",
    "y = np.linspace(-2, 2, 20)\n",
    "\n",
    "# Create meshgrid\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Define function f(x, y) = exp(-x^2 - y^2)\n",
    "F = np.exp(-(X**2 + Y**2))\n",
    "\n",
    "# Compute numerical gradient\n",
    "# Computes gradient with respect to x and y\n",
    "dFdx, dFdy = np.gradient(F, x, y) \n",
    "\n",
    "# Plot function\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.contourf(X, Y, F, levels=20, cmap=\"plasma\")  # Filled contour plot of f(x,y)\n",
    "ax.quiver(X, Y, dFdx, dFdy, color='white')  # Quiver plot of gradient vectors\n",
    "ax.set_title(\"Gradient of a Gaussian Function\")\n",
    "ax.set_xlabel(\"x-axis\")\n",
    "ax.set_ylabel(\"y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the roundoff errors is by simply using the **decimal** package, which provides a higher precision floating point number representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "dx = Decimal(\"1.\")\n",
    "while(dx >= Decimal(\"1.e-10\")):\n",
    "    x = Decimal(\"1.\")\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-Decimal(\"2.\")))\n",
    "    dx = dx / Decimal(\"10.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better than numerical differentiation is automatic differentiation or *autodiff*, which is crucial to breakthroughs in machine learning.\n",
    "\n",
    "This is a technique that allows to evaluate the derivative of a function to machine precision, without the need to use finite differences, using the fact that autodiff package knows the analytical form of the derivative for certain functions. It then builds a computational graph that allows for the evaluation of the derivative of a function using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy\n",
    "`sympy`, the symbolic mathematics library, allows differentiation of expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = x**2 + 3*x + 5\n",
    "dfdx = sp.diff(f, x)\n",
    "\n",
    "print(dfdx)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd (may need to install)\n",
    "`autograd` is a Python library that enables automatic differentiation of native Python and NumPy functions. It is lightweight and useful for computing derivatives of functions without needing to manually derive them.\n",
    "Some key features of `autograd`:\n",
    "- Computes gradients automatically.\n",
    "- Works with standard Python and NumPy functions.\n",
    "- Supports higher-order derivatives (e.g., second-order derivatives).\n",
    "- Can differentiate through loops, branches, and recursion.\n",
    "\n",
    "You should think about using `autograd`:\n",
    "- If you need automatic differentiation but don’t want to manually compute derivatives.\n",
    "- If you're working with NumPy-based code (since jax require different frameworks).\n",
    "- If you need higher-order derivatives, like Hessians and Jacobians.\n",
    "\n",
    "Please note some limitations of `autograd`\n",
    "- No support for control flow like if statements depending on values. (JAX solves this.)\n",
    "- Slower than JAX or TensorFlow for large-scale computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd as ag\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 5\n",
    "\n",
    "dfdx = ag.grad(f)  # Get the gradient function\n",
    "print(dfdx(2.0))  # Evaluate derivative at x=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` can perform partial derivatives, calculate jacobians, and compute hessian matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y**3\n",
    "\n",
    "dfdx = ag.grad(f, argnum=0)  # ∂f/∂x\n",
    "dfdy = ag.grad(f, argnum=1)  # ∂f/∂y\n",
    "\n",
    "print(dfdx(2.0, 3.0))  \n",
    "print(dfdy(2.0, 3.0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb(x):\n",
    "    return np.array([x[0]**2, x[1]**3])  # Vector-valued function\n",
    "\n",
    "jacob = ag.jacobian(jb)\n",
    "print(jacob(np.array([2.0, 3.0])))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hs(x):\n",
    "    return x[0]**2 + x[1]**3\n",
    "\n",
    "hess = ag.hessian(hs)\n",
    "print(hess(np.array([1.0, 2.0])))  # Output: Hessian matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = jax.grad(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 13.0\n",
    "print(f\"The gradient of f at x = {x} is {grad_f(x):.20f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare this to the finite difference technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 13.\n",
    "while(dx > 1.e-10):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f(13.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no error on the autodiff result because it knows $dy/dx$ as a *function*, rather than computing it by numerical approximation\n",
    "\n",
    "Ok, so that's great and all, but autodiff really gets its legs when you have a more complicated function. Derivatives of a more complicated function that is composed of many smaller functions require many applications of the chain rule. Autodiff does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_complicated(x):\n",
    "    return jnp.cos(jnp.sin(jnp.tanh(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3.14,100)\n",
    "plt.plot(x,f_complicated(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fcomp = jax.grad(f_complicated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3.14,100)\n",
    "plt.plot(x,[grad_fcomp(j) for j in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fcomp(3.1415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most complicated functions out there are called *neural networks* which can involve millions or billions of smaller functions or *neurons* in the composition. Autodiff also works on them, which is one of the crucial reasons there has been so much progress in AI in the last 10 years. \n",
    "\n",
    "Let's recall our simple neural network architecture that we studied prevously, a feedforward neural network of depth $3$. The equation defining the network is given by:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{W}_3\\sigma(\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{x}+\\mathbf{b}_1)+\\mathbf{b}_2)+\\mathbf{b}_3$$\n",
    "\n",
    "where $\\mathbf{x}$ is the input vector, $\\mathbf{y}$ is the neural network prediction, $\\mathbf{W}_i$ and $\\mathbf{b}_i$ are the weight matrices and bias vectors of the network, and $\\sigma$ is the non-linear activation function. Recall that the weights and biases are the parameters that are updated when the network is trained to do something useful.\n",
    "\n",
    "So why is autodiff important here? This function looks complicated, but it's just a composition of affine transformations and elementwise non-linearities. We know the derivative of each of these functions, so we can use the chain rule to compute the derivative of the entire network, which is crucial for training. For functions that are compositions of many smaller functions, this is much more efficient than using finite differences and it has less error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
